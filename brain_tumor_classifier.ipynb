{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee1cb8cc-b9fa-44ef-9611-1c3dbeb1b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340b57c2-41f5-4d4c-ac2b-532cde0c40db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loc = 'dataset'\n",
    "IMG_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d78fb3c-9a13-400c-ac23-a35fed266961",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbdf389-3d41-4c82-a436-59e5a199112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "training_path = os.path.join(dataset_loc, 'Training') \n",
    "\n",
    "for category in categories:\n",
    "    cancer_path = os.path.join(training_path, category)\n",
    "    category_idx = categories.index(category)\n",
    "    for img_file in os.listdir(cancer_path):\n",
    "        try:\n",
    "            img_array = cv2.imread(os.path.join(cancer_path, img_file), cv2.IMREAD_GRAYSCALE) # convert image to gray scale\n",
    "            img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE)) # all images same size\n",
    "            img_array = tf.keras.utils.normalize(img_array)\n",
    "\n",
    "            training_data.append([img_array, category_idx])\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0945eafa-9e4b-4a74-a1ee-528e4bb116f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = []\n",
    "validation_path = os.path.join(dataset_loc, 'Testing') \n",
    "\n",
    "for category in categories:\n",
    "    cancer_path = os.path.join(validation_path, category)\n",
    "    category_idx = categories.index(category)\n",
    "    for img_file in os.listdir(cancer_path):\n",
    "        try:\n",
    "            img_array = cv2.imread(os.path.join(cancer_path, img_file), cv2.IMREAD_GRAYSCALE) # convert image to gray scale\n",
    "            img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE)) # all images same size\n",
    "            img_array = tf.keras.utils.normalize(img_array)\n",
    "            \n",
    "            validation_data.append([img_array, category_idx])\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ea3d407-1aa4-4a76-bb4f-61ed29eac8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_data)\n",
    "random.shuffle(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8057c096-c2ee-4934-aa2c-297e980c8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [pair[0] for pair in training_data]\n",
    "y_train = [pair[1] for pair in training_data]\n",
    "\n",
    "X_validation = [pair[0] for pair in validation_data]\n",
    "y_validation = [pair[1] for pair in validation_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a447bc-76d6-4371-8a33-8a9307f35347",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train) # training and validation sets must be converted to np_arrays for cnn model\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_validation = np.array(X_validation)\n",
    "y_validation = np.array(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313a3420-12c3-4b0a-bda2-e34b966df906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 23:08:37.616523: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Create Keras model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten()) # Dense layers require 1D array (np.reshape may also be used)\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=128, activation='relu')) # relu is a good one to fall back on \n",
    "model.add(Dense(units=4, activation='softmax')) # softmax used for probability distribution\n",
    "\n",
    "# model.add(Conv2D(filters=32,kernel_size=(3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Flatten()) # Dense layers require 1D array (np.reshape may also be used)\n",
    "# model.add(Dense(256,activation='relu'))\n",
    "# model.add(Dense(4,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy']) # If your labels are ints representing class: Use sparse_categorical_crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "344164bf-c9a2-43ce-864e-37bd6a3df646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.9450 - accuracy: 0.6028\n",
      "Epoch 2/20\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.6007 - accuracy: 0.7620\n",
      "Epoch 3/20\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.4794 - accuracy: 0.8111\n",
      "Epoch 4/20\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.3416 - accuracy: 0.8690\n",
      "Epoch 5/20\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 0.2699 - accuracy: 0.9042\n",
      "Epoch 6/20\n",
      "90/90 [==============================] - 1s 16ms/step - loss: 0.2658 - accuracy: 0.9014\n",
      "Epoch 7/20\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 0.2247 - accuracy: 0.9132\n",
      "Epoch 8/20\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.1259 - accuracy: 0.9592\n",
      "Epoch 9/20\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.1302 - accuracy: 0.9547\n",
      "Epoch 10/20\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.1450 - accuracy: 0.9467\n",
      "Epoch 11/20\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.1146 - accuracy: 0.9547\n",
      "Epoch 12/20\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.0715 - accuracy: 0.9746\n",
      "Epoch 13/20\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.0864 - accuracy: 0.9679\n",
      "Epoch 14/20\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0380 - accuracy: 0.9895\n",
      "Epoch 15/20\n",
      "90/90 [==============================] - 1s 15ms/step - loss: 0.0627 - accuracy: 0.9777\n",
      "Epoch 16/20\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0293 - accuracy: 0.9927\n",
      "Epoch 17/20\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0563 - accuracy: 0.9774\n",
      "Epoch 18/20\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 0.0553 - accuracy: 0.9815\n",
      "Epoch 19/20\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0414 - accuracy: 0.9840\n",
      "Epoch 20/20\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0237 - accuracy: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x139ad6fb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb730406-8463-4f2d-920a-7eab5a57f1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 6ms/step - loss: 3.8117 - accuracy: 0.7360\n"
     ]
    }
   ],
   "source": [
    "validation_loss, validation_acc = model.evaluate(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff82587a-11b2-4b3a-838f-9a15567de9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c9ad2-3304-45a5-a6cd-82bc9a1b1713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
